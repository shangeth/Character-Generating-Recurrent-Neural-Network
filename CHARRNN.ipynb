{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CHARRNN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OnLnxMA1Wemj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Download\n"
      ]
    },
    {
      "metadata": {
        "id": "7LUW2C3mWg7z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cfa82723-9c27-4061-e628-449d8423b458"
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/recurrent-neural-networks/char-rnn/data/anna.txt\n",
        "!mv /content/anna.txt /content/data.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-24 09:36:45--  https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/recurrent-neural-networks/char-rnn/data/anna.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2025486 (1.9M) [text/plain]\n",
            "Saving to: ‘anna.txt’\n",
            "\n",
            "\ranna.txt              0%[                    ]       0  --.-KB/s               \ranna.txt            100%[===================>]   1.93M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2019-03-24 09:36:45 (26.1 MB/s) - ‘anna.txt’ saved [2025486/2025486]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LA05UTorW2Lz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/udacity/deep-learning-v2-pytorch/raw/9b6001a7163e94eda9941ae448ce522eb7ba3a7d/recurrent-neural-networks/char-rnn/assets/charseq.jpeg)"
      ]
    },
    {
      "metadata": {
        "id": "mIIxP_4aW7rF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Process"
      ]
    },
    {
      "metadata": {
        "id": "ztFD_tsjWiOA",
        "colab_type": "code",
        "outputId": "4cd8badb-40e9-4aaa-8309-ae8ec3de5099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "DATA_FILE = '/content/data.txt'\n",
        "\n",
        "with open(DATA_FILE, 'r') as f:\n",
        "  text = f.read()\n",
        "  \n",
        "text[:200]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverything was in confusion in the Oblonskys' house. The wife had\\ndiscovered that the husband was carrying on\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "yw38z69rXnv6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenize data"
      ]
    },
    {
      "metadata": {
        "id": "U-lzalWBXeOb",
        "colab_type": "code",
        "outputId": "8ee325c0-1983-4a82-f271-35640e7999cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "char = list(set(text))\n",
        "int2char = dict(enumerate(char))\n",
        "char2int = {ch: i for i, ch in int2char.items()}\n",
        "\n",
        "encoded = np.array([char2int[ch] for ch in text])\n",
        "print(text[:100])\n",
        "print('\\nis encoded as \\n',encoded[:100])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter 1\n",
            "\n",
            "\n",
            "Happy families are all alike; every unhappy family is unhappy in its own\n",
            "way.\n",
            "\n",
            "Everythin\n",
            "\n",
            "is encoded as \n",
            " [55 23 62 80 69  0 47 75 60 82 82 82 56 62 80 80 27 75 53 62 18 19 74 19\n",
            "  0 13 75 62 47  0 75 62 74 74 75 62 74 19 81  0 25 75  0 64  0 47 27 75\n",
            " 32 48 23 62 80 80 27 75 53 62 18 19 74 27 75 19 13 75 32 48 23 62 80 80\n",
            " 27 75 19 48 75 19 69 13 75 52 36 48 82 36 62 27  1 82 82 26 64  0 47 27\n",
            " 69 23 19 48]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nP4o7VYWENZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d38a29cf-a4ba-42d0-e215-2a83720beb22"
      },
      "cell_type": "code",
      "source": [
        "len(char)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "8b2k35pYakO_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## One-Hot encode chars"
      ]
    },
    {
      "metadata": {
        "id": "L3AXLrJkaN_F",
        "colab_type": "code",
        "outputId": "f6183f6b-2f7c-4bd5-cd75-3f6b1b2c2467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "def onehot(arr, nlabels):\n",
        "  one_hot = np.zeros((np.multiply(*arr.shape), nlabels), dtype=np.float32)\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
        "  one_hot = one_hot.reshape((*arr.shape, nlabels))\n",
        "  return one_hot\n",
        "\n",
        "print(onehot(np.array([[1,2,3]]), 4))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 1. 0. 0.]\n",
            "  [0. 0. 1. 0.]\n",
            "  [0. 0. 0. 1.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eo2vBIwZeRK0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Batch Generator"
      ]
    },
    {
      "metadata": {
        "id": "K36ORF9wbzNR",
        "colab_type": "code",
        "outputId": "49e80792-fd7e-407d-a13b-104fb42c2f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "def batcher(arr, batch_size, seq_len):\n",
        "  n_batches = len(arr)//(batch_size * seq_len)\n",
        "  arr = arr[:(n_batches * batch_size * seq_len)]\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "  for n in range(0, arr.shape[1], seq_len):\n",
        "    x = arr[:, n:n+seq_len]\n",
        "    y = np.zeros_like(x)\n",
        "    try:\n",
        "        y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_len]\n",
        "    except IndexError:\n",
        "        y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "    yield x, y\n",
        "    \n",
        "    \n",
        "for x,y in batcher(np.arange(36),6, 3):\n",
        "  print(x,'\\n')\n",
        "  print(y)\n",
        "  break"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2]\n",
            " [ 6  7  8]\n",
            " [12 13 14]\n",
            " [18 19 20]\n",
            " [24 25 26]\n",
            " [30 31 32]] \n",
            "\n",
            "[[ 1  2  3]\n",
            " [ 7  8  9]\n",
            " [13 14 15]\n",
            " [19 20 21]\n",
            " [25 26 27]\n",
            " [31 32 33]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JXOSk3KKh1kw",
        "colab_type": "code",
        "outputId": "787d578f-d51d-4acd-8b09-ec1f6fb235ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "batches = batcher(encoded, 8, 50)\n",
        "x, y = next(batches)\n",
        "\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[55 23 62 80 69  0 47 75 60 82]\n",
            " [13 52 48 75 69 23 62 69 75 62]\n",
            " [ 0 48  4 75 52 47 75 62 75 53]\n",
            " [13 75 69 23  0 75 57 23 19  0]\n",
            " [75 13 62 36 75 23  0 47 75 69]\n",
            " [57 32 13 13 19 52 48 75 62 48]\n",
            " [75 37 48 48 62 75 23 62  4 75]\n",
            " [ 3 35 74 52 48 13 81 27  1 75]]\n",
            "\n",
            "y\n",
            " [[23 62 80 69  0 47 75 60 82 82]\n",
            " [52 48 75 69 23 62 69 75 62 69]\n",
            " [48  4 75 52 47 75 62 75 53 52]\n",
            " [75 69 23  0 75 57 23 19  0 53]\n",
            " [13 62 36 75 23  0 47 75 69  0]\n",
            " [32 13 13 19 52 48 75 62 48  4]\n",
            " [37 48 48 62 75 23 62  4 75 13]\n",
            " [35 74 52 48 13 81 27  1 75 54]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ODqHgre2pyUf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "![](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charRNN.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "0BaE4XY7pbQR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CharacterRNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "    super(CharacterRNN, self).__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "\n",
        "    self.chars = tokens\n",
        "    self.int2char = dict(enumerate(self.chars))\n",
        "    self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        \n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)    \n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc1 = nn.Linear(n_hidden, 128)\n",
        "    self.fc2 = nn.Linear(128, len(self.chars))\n",
        "    self.relu = nn.ReLU()\n",
        "    \n",
        "  def forward(self, x, hidden):\n",
        "    r_output, hidden = self.lstm(x, hidden)\n",
        "    out = self.dropout(r_output)\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    return out, hidden\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "              weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ptlEMxaDwr4C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training loop\n"
      ]
    },
    {
      "metadata": {
        "id": "tD6FRNCfuqif",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in batcher(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            x = onehot(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            net.zero_grad()\n",
        "            \n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            if counter % print_every == 0:\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in batcher(val_data, batch_size, seq_length):\n",
        "                    x = onehot(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train()\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Train Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5bLo0On-wugJ",
        "colab_type": "code",
        "outputId": "e47d4ba1-f1c7-4da3-881b-1c241857816f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharacterRNN(char, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharacterRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5)\n",
            "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=83, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hn4Rkgivwxjb",
        "colab_type": "code",
        "outputId": "734761ce-7bc4-4665-af8a-e430b5dee8c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 1500\n",
        "seq_length = 100\n",
        "n_epochs = 50 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 10... Train Loss: 3.3373... Val Loss: 3.3239\n",
            "Epoch: 2/50... Step: 20... Train Loss: 3.1536... Val Loss: 3.1611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FbkrY7Wb9p5u",
        "colab_type": "code",
        "outputId": "ddd1cfc5-9010-4321-fa82-ed824dd01be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "cell_type": "code",
      "source": [
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 10... Loss: 1.7567... Val Loss: 1.7216\n",
            "Epoch: 2/50... Step: 20... Loss: 1.7247... Val Loss: 1.6844\n",
            "Epoch: 3/50... Step: 30... Loss: 1.6993... Val Loss: 1.6639\n",
            "Epoch: 4/50... Step: 40... Loss: 1.6917... Val Loss: 1.6521\n",
            "Epoch: 5/50... Step: 50... Loss: 1.6679... Val Loss: 1.6427\n",
            "Epoch: 6/50... Step: 60... Loss: 1.6583... Val Loss: 1.6340\n",
            "Epoch: 7/50... Step: 70... Loss: 1.6429... Val Loss: 1.6263\n",
            "Epoch: 8/50... Step: 80... Loss: 1.6508... Val Loss: 1.6208\n",
            "Epoch: 9/50... Step: 90... Loss: 1.6338... Val Loss: 1.6153\n",
            "Epoch: 10/50... Step: 100... Loss: 1.6679... Val Loss: 1.6081\n",
            "Epoch: 10/50... Step: 110... Loss: 1.6552... Val Loss: 1.6003\n",
            "Epoch: 11/50... Step: 120... Loss: 1.6079... Val Loss: 1.5955\n",
            "Epoch: 12/50... Step: 130... Loss: 1.6118... Val Loss: 1.5860\n",
            "Epoch: 13/50... Step: 140... Loss: 1.5988... Val Loss: 1.5790\n",
            "Epoch: 14/50... Step: 150... Loss: 1.5980... Val Loss: 1.5738\n",
            "Epoch: 15/50... Step: 160... Loss: 1.5824... Val Loss: 1.5670\n",
            "Epoch: 16/50... Step: 170... Loss: 1.5739... Val Loss: 1.5604\n",
            "Epoch: 17/50... Step: 180... Loss: 1.5557... Val Loss: 1.5538\n",
            "Epoch: 18/50... Step: 190... Loss: 1.5630... Val Loss: 1.5465\n",
            "Epoch: 19/50... Step: 200... Loss: 1.5490... Val Loss: 1.5384\n",
            "Epoch: 20/50... Step: 210... Loss: 1.5861... Val Loss: 1.5332\n",
            "Epoch: 20/50... Step: 220... Loss: 1.5732... Val Loss: 1.5251\n",
            "Epoch: 21/50... Step: 230... Loss: 1.5198... Val Loss: 1.5204\n",
            "Epoch: 22/50... Step: 240... Loss: 1.5247... Val Loss: 1.5121\n",
            "Epoch: 23/50... Step: 250... Loss: 1.5122... Val Loss: 1.5073\n",
            "Epoch: 24/50... Step: 260... Loss: 1.5126... Val Loss: 1.5017\n",
            "Epoch: 25/50... Step: 270... Loss: 1.4992... Val Loss: 1.4976\n",
            "Epoch: 26/50... Step: 280... Loss: 1.4869... Val Loss: 1.4914\n",
            "Epoch: 27/50... Step: 290... Loss: 1.4728... Val Loss: 1.4860\n",
            "Epoch: 28/50... Step: 300... Loss: 1.4780... Val Loss: 1.4788\n",
            "Epoch: 29/50... Step: 310... Loss: 1.4667... Val Loss: 1.4756\n",
            "Epoch: 30/50... Step: 320... Loss: 1.5037... Val Loss: 1.4670\n",
            "Epoch: 30/50... Step: 330... Loss: 1.4964... Val Loss: 1.4609\n",
            "Epoch: 31/50... Step: 340... Loss: 1.4399... Val Loss: 1.4591\n",
            "Epoch: 32/50... Step: 350... Loss: 1.4436... Val Loss: 1.4523\n",
            "Epoch: 33/50... Step: 360... Loss: 1.4364... Val Loss: 1.4480\n",
            "Epoch: 34/50... Step: 370... Loss: 1.4341... Val Loss: 1.4434\n",
            "Epoch: 35/50... Step: 380... Loss: 1.4221... Val Loss: 1.4395\n",
            "Epoch: 36/50... Step: 390... Loss: 1.4127... Val Loss: 1.4366\n",
            "Epoch: 37/50... Step: 400... Loss: 1.4015... Val Loss: 1.4314\n",
            "Epoch: 38/50... Step: 410... Loss: 1.4119... Val Loss: 1.4284\n",
            "Epoch: 39/50... Step: 420... Loss: 1.3988... Val Loss: 1.4338\n",
            "Epoch: 40/50... Step: 430... Loss: 1.4434... Val Loss: 1.4173\n",
            "Epoch: 40/50... Step: 440... Loss: 1.4323... Val Loss: 1.4155\n",
            "Epoch: 41/50... Step: 450... Loss: 1.3754... Val Loss: 1.4118\n",
            "Epoch: 42/50... Step: 460... Loss: 1.3803... Val Loss: 1.4070\n",
            "Epoch: 43/50... Step: 470... Loss: 1.3722... Val Loss: 1.4050\n",
            "Epoch: 44/50... Step: 480... Loss: 1.3769... Val Loss: 1.4016\n",
            "Epoch: 45/50... Step: 490... Loss: 1.3637... Val Loss: 1.3989\n",
            "Epoch: 46/50... Step: 500... Loss: 1.3570... Val Loss: 1.3950\n",
            "Epoch: 47/50... Step: 510... Loss: 1.3433... Val Loss: 1.3941\n",
            "Epoch: 48/50... Step: 520... Loss: 1.3571... Val Loss: 1.3886\n",
            "Epoch: 49/50... Step: 530... Loss: 1.3459... Val Loss: 1.3862\n",
            "Epoch: 50/50... Step: 540... Loss: 1.3888... Val Loss: 1.3842\n",
            "Epoch: 50/50... Step: 550... Loss: 1.3835... Val Loss: 1.3807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kwG2SC0Y-jbU",
        "colab_type": "code",
        "outputId": "09aa8fdd-9304-40f3-b24d-46a9820a55c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "cell_type": "code",
      "source": [
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 10... Loss: 1.3621... Val Loss: 1.3971\n",
            "Epoch: 2/50... Step: 20... Loss: 1.3452... Val Loss: 1.3813\n",
            "Epoch: 3/50... Step: 30... Loss: 1.3278... Val Loss: 1.3716\n",
            "Epoch: 4/50... Step: 40... Loss: 1.3272... Val Loss: 1.3676\n",
            "Epoch: 5/50... Step: 50... Loss: 1.3135... Val Loss: 1.3655\n",
            "Epoch: 6/50... Step: 60... Loss: 1.3075... Val Loss: 1.3634\n",
            "Epoch: 7/50... Step: 70... Loss: 1.2980... Val Loss: 1.3614\n",
            "Epoch: 8/50... Step: 80... Loss: 1.3082... Val Loss: 1.3594\n",
            "Epoch: 9/50... Step: 90... Loss: 1.3035... Val Loss: 1.3576\n",
            "Epoch: 10/50... Step: 100... Loss: 1.3472... Val Loss: 1.3560\n",
            "Epoch: 10/50... Step: 110... Loss: 1.3454... Val Loss: 1.3534\n",
            "Epoch: 11/50... Step: 120... Loss: 1.2875... Val Loss: 1.3526\n",
            "Epoch: 12/50... Step: 130... Loss: 1.2964... Val Loss: 1.3528\n",
            "Epoch: 13/50... Step: 140... Loss: 1.2884... Val Loss: 1.3494\n",
            "Epoch: 14/50... Step: 150... Loss: 1.2936... Val Loss: 1.3480\n",
            "Epoch: 15/50... Step: 160... Loss: 1.2831... Val Loss: 1.3488\n",
            "Epoch: 16/50... Step: 170... Loss: 1.2792... Val Loss: 1.3489\n",
            "Epoch: 17/50... Step: 180... Loss: 1.2691... Val Loss: 1.3450\n",
            "Epoch: 18/50... Step: 190... Loss: 1.2803... Val Loss: 1.3428\n",
            "Epoch: 19/50... Step: 200... Loss: 1.2743... Val Loss: 1.3412\n",
            "Epoch: 20/50... Step: 210... Loss: 1.3154... Val Loss: 1.3401\n",
            "Epoch: 20/50... Step: 220... Loss: 1.3159... Val Loss: 1.3376\n",
            "Epoch: 21/50... Step: 230... Loss: 1.2606... Val Loss: 1.3370\n",
            "Epoch: 22/50... Step: 240... Loss: 1.2656... Val Loss: 1.3356\n",
            "Epoch: 23/50... Step: 250... Loss: 1.2617... Val Loss: 1.3347\n",
            "Epoch: 24/50... Step: 260... Loss: 1.2645... Val Loss: 1.3337\n",
            "Epoch: 25/50... Step: 270... Loss: 1.2513... Val Loss: 1.3304\n",
            "Epoch: 26/50... Step: 280... Loss: 1.2527... Val Loss: 1.3314\n",
            "Epoch: 27/50... Step: 290... Loss: 1.2420... Val Loss: 1.3320\n",
            "Epoch: 28/50... Step: 300... Loss: 1.2500... Val Loss: 1.3267\n",
            "Epoch: 29/50... Step: 310... Loss: 1.2475... Val Loss: 1.3260\n",
            "Epoch: 30/50... Step: 320... Loss: 1.2933... Val Loss: 1.3255\n",
            "Epoch: 30/50... Step: 330... Loss: 1.2906... Val Loss: 1.3241\n",
            "Epoch: 31/50... Step: 340... Loss: 1.2353... Val Loss: 1.3216\n",
            "Epoch: 32/50... Step: 350... Loss: 1.2388... Val Loss: 1.3255\n",
            "Epoch: 33/50... Step: 360... Loss: 1.2372... Val Loss: 1.3188\n",
            "Epoch: 34/50... Step: 370... Loss: 1.2367... Val Loss: 1.3210\n",
            "Epoch: 35/50... Step: 380... Loss: 1.2293... Val Loss: 1.3197\n",
            "Epoch: 36/50... Step: 390... Loss: 1.2236... Val Loss: 1.3176\n",
            "Epoch: 37/50... Step: 400... Loss: 1.2147... Val Loss: 1.3176\n",
            "Epoch: 38/50... Step: 410... Loss: 1.2272... Val Loss: 1.3162\n",
            "Epoch: 39/50... Step: 420... Loss: 1.2218... Val Loss: 1.3204\n",
            "Epoch: 40/50... Step: 430... Loss: 1.2774... Val Loss: 1.3109\n",
            "Epoch: 40/50... Step: 440... Loss: 1.2627... Val Loss: 1.3131\n",
            "Epoch: 41/50... Step: 450... Loss: 1.2039... Val Loss: 1.3140\n",
            "Epoch: 42/50... Step: 460... Loss: 1.2119... Val Loss: 1.3093\n",
            "Epoch: 43/50... Step: 470... Loss: 1.2074... Val Loss: 1.3096\n",
            "Epoch: 44/50... Step: 480... Loss: 1.2084... Val Loss: 1.3083\n",
            "Epoch: 45/50... Step: 490... Loss: 1.2032... Val Loss: 1.3118\n",
            "Epoch: 46/50... Step: 500... Loss: 1.2064... Val Loss: 1.3099\n",
            "Epoch: 47/50... Step: 510... Loss: 1.1933... Val Loss: 1.3080\n",
            "Epoch: 48/50... Step: 520... Loss: 1.2031... Val Loss: 1.3132\n",
            "Epoch: 49/50... Step: 530... Loss: 1.1934... Val Loss: 1.3041\n",
            "Epoch: 50/50... Step: 540... Loss: 1.2560... Val Loss: 1.3096\n",
            "Epoch: 50/50... Step: 550... Loss: 1.2417... Val Loss: 1.3033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UGfI0MCxGq6T",
        "colab_type": "code",
        "outputId": "a374b08f-3f89-497a-f959-c6ad567c04a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "cell_type": "code",
      "source": [
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.01, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 10... Loss: 2.4489... Val Loss: 2.2425\n",
            "Epoch: 2/50... Step: 20... Loss: 2.1286... Val Loss: 1.9960\n",
            "Epoch: 3/50... Step: 30... Loss: 1.9278... Val Loss: 1.8161\n",
            "Epoch: 4/50... Step: 40... Loss: 1.7454... Val Loss: 1.6524\n",
            "Epoch: 5/50... Step: 50... Loss: 1.5961... Val Loss: 1.5337\n",
            "Epoch: 6/50... Step: 60... Loss: 1.4860... Val Loss: 1.4587\n",
            "Epoch: 7/50... Step: 70... Loss: 1.4121... Val Loss: 1.4166\n",
            "Epoch: 8/50... Step: 80... Loss: 1.3875... Val Loss: 1.3887\n",
            "Epoch: 9/50... Step: 90... Loss: 1.3477... Val Loss: 1.3676\n",
            "Epoch: 10/50... Step: 100... Loss: 1.3630... Val Loss: 1.3591\n",
            "Epoch: 10/50... Step: 110... Loss: 1.3514... Val Loss: 1.3462\n",
            "Epoch: 11/50... Step: 120... Loss: 1.2825... Val Loss: 1.3387\n",
            "Epoch: 12/50... Step: 130... Loss: 1.2804... Val Loss: 1.3301\n",
            "Epoch: 13/50... Step: 140... Loss: 1.2675... Val Loss: 1.3295\n",
            "Epoch: 14/50... Step: 150... Loss: 1.2601... Val Loss: 1.3211\n",
            "Epoch: 15/50... Step: 160... Loss: 1.2433... Val Loss: 1.3174\n",
            "Epoch: 16/50... Step: 170... Loss: 1.2403... Val Loss: 1.3150\n",
            "Epoch: 17/50... Step: 180... Loss: 1.2279... Val Loss: 1.3111\n",
            "Epoch: 18/50... Step: 190... Loss: 1.2339... Val Loss: 1.3113\n",
            "Epoch: 19/50... Step: 200... Loss: 1.2203... Val Loss: 1.3104\n",
            "Epoch: 20/50... Step: 210... Loss: 1.2662... Val Loss: 1.3020\n",
            "Epoch: 20/50... Step: 220... Loss: 1.2562... Val Loss: 1.3026\n",
            "Epoch: 21/50... Step: 230... Loss: 1.1951... Val Loss: 1.2973\n",
            "Epoch: 22/50... Step: 240... Loss: 1.2045... Val Loss: 1.2954\n",
            "Epoch: 23/50... Step: 250... Loss: 1.1925... Val Loss: 1.2950\n",
            "Epoch: 24/50... Step: 260... Loss: 1.1934... Val Loss: 1.2937\n",
            "Epoch: 25/50... Step: 270... Loss: 1.1798... Val Loss: 1.2915\n",
            "Epoch: 26/50... Step: 280... Loss: 1.1780... Val Loss: 1.2950\n",
            "Epoch: 27/50... Step: 290... Loss: 1.1686... Val Loss: 1.2870\n",
            "Epoch: 28/50... Step: 300... Loss: 1.1746... Val Loss: 1.2903\n",
            "Epoch: 29/50... Step: 310... Loss: 1.1736... Val Loss: 1.2797\n",
            "Epoch: 30/50... Step: 320... Loss: 1.2220... Val Loss: 1.2871\n",
            "Epoch: 30/50... Step: 330... Loss: 1.2135... Val Loss: 1.2832\n",
            "Epoch: 31/50... Step: 340... Loss: 1.1477... Val Loss: 1.2823\n",
            "Epoch: 32/50... Step: 350... Loss: 1.1567... Val Loss: 1.2891\n",
            "Epoch: 33/50... Step: 360... Loss: 1.1559... Val Loss: 1.2805\n",
            "Epoch: 34/50... Step: 370... Loss: 1.1554... Val Loss: 1.2794\n",
            "Epoch: 35/50... Step: 380... Loss: 1.1425... Val Loss: 1.2852\n",
            "Epoch: 36/50... Step: 390... Loss: 1.1395... Val Loss: 1.2771\n",
            "Epoch: 37/50... Step: 400... Loss: 1.1295... Val Loss: 1.2777\n",
            "Epoch: 38/50... Step: 410... Loss: 1.1376... Val Loss: 1.2787\n",
            "Epoch: 39/50... Step: 420... Loss: 1.1346... Val Loss: 1.2733\n",
            "Epoch: 40/50... Step: 430... Loss: 1.1921... Val Loss: 1.2774\n",
            "Epoch: 40/50... Step: 440... Loss: 1.1744... Val Loss: 1.2761\n",
            "Epoch: 41/50... Step: 450... Loss: 1.1105... Val Loss: 1.2792\n",
            "Epoch: 42/50... Step: 460... Loss: 1.1161... Val Loss: 1.2800\n",
            "Epoch: 43/50... Step: 470... Loss: 1.1183... Val Loss: 1.2711\n",
            "Epoch: 44/50... Step: 480... Loss: 1.1187... Val Loss: 1.2711\n",
            "Epoch: 45/50... Step: 490... Loss: 1.1098... Val Loss: 1.2766\n",
            "Epoch: 46/50... Step: 500... Loss: 1.1092... Val Loss: 1.2728\n",
            "Epoch: 47/50... Step: 510... Loss: 1.1020... Val Loss: 1.2760\n",
            "Epoch: 48/50... Step: 520... Loss: 1.1078... Val Loss: 1.2736\n",
            "Epoch: 49/50... Step: 530... Loss: 1.1010... Val Loss: 1.2662\n",
            "Epoch: 50/50... Step: 540... Loss: 1.1678... Val Loss: 1.2716\n",
            "Epoch: 50/50... Step: 550... Loss: 1.1451... Val Loss: 1.2729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H2nmcEVfKD0A",
        "colab_type": "code",
        "outputId": "db8cf4ac-5f2e-4676-d259-5730b2ecbeea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "cell_type": "code",
      "source": [
        "train(net, encoded, epochs=50, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 10... Loss: 1.0796... Val Loss: 1.2668\n",
            "Epoch: 2/50... Step: 20... Loss: 1.0784... Val Loss: 1.2690\n",
            "Epoch: 3/50... Step: 30... Loss: 1.0722... Val Loss: 1.2702\n",
            "Epoch: 4/50... Step: 40... Loss: 1.0758... Val Loss: 1.2699\n",
            "Epoch: 5/50... Step: 50... Loss: 1.0693... Val Loss: 1.2699\n",
            "Epoch: 6/50... Step: 60... Loss: 1.0661... Val Loss: 1.2680\n",
            "Epoch: 7/50... Step: 70... Loss: 1.0617... Val Loss: 1.2678\n",
            "Epoch: 8/50... Step: 80... Loss: 1.0648... Val Loss: 1.2672\n",
            "Epoch: 9/50... Step: 90... Loss: 1.0639... Val Loss: 1.2662\n",
            "Epoch: 10/50... Step: 100... Loss: 1.1284... Val Loss: 1.2685\n",
            "Epoch: 10/50... Step: 110... Loss: 1.1154... Val Loss: 1.2699\n",
            "Epoch: 11/50... Step: 120... Loss: 1.0544... Val Loss: 1.2713\n",
            "Epoch: 12/50... Step: 130... Loss: 1.0573... Val Loss: 1.2715\n",
            "Epoch: 13/50... Step: 140... Loss: 1.0565... Val Loss: 1.2712\n",
            "Epoch: 14/50... Step: 150... Loss: 1.0603... Val Loss: 1.2722\n",
            "Epoch: 15/50... Step: 160... Loss: 1.0543... Val Loss: 1.2702\n",
            "Epoch: 16/50... Step: 170... Loss: 1.0523... Val Loss: 1.2695\n",
            "Epoch: 17/50... Step: 180... Loss: 1.0467... Val Loss: 1.2674\n",
            "Epoch: 18/50... Step: 190... Loss: 1.0531... Val Loss: 1.2683\n",
            "Epoch: 19/50... Step: 200... Loss: 1.0480... Val Loss: 1.2700\n",
            "Epoch: 20/50... Step: 210... Loss: 1.1168... Val Loss: 1.2697\n",
            "Epoch: 20/50... Step: 220... Loss: 1.1045... Val Loss: 1.2735\n",
            "Epoch: 21/50... Step: 230... Loss: 1.0425... Val Loss: 1.2734\n",
            "Epoch: 22/50... Step: 240... Loss: 1.0456... Val Loss: 1.2732\n",
            "Epoch: 23/50... Step: 250... Loss: 1.0487... Val Loss: 1.2735\n",
            "Epoch: 24/50... Step: 260... Loss: 1.0497... Val Loss: 1.2733\n",
            "Epoch: 25/50... Step: 270... Loss: 1.0464... Val Loss: 1.2715\n",
            "Epoch: 26/50... Step: 280... Loss: 1.0406... Val Loss: 1.2713\n",
            "Epoch: 27/50... Step: 290... Loss: 1.0367... Val Loss: 1.2712\n",
            "Epoch: 28/50... Step: 300... Loss: 1.0425... Val Loss: 1.2715\n",
            "Epoch: 29/50... Step: 310... Loss: 1.0407... Val Loss: 1.2726\n",
            "Epoch: 30/50... Step: 320... Loss: 1.1092... Val Loss: 1.2731\n",
            "Epoch: 30/50... Step: 330... Loss: 1.0942... Val Loss: 1.2742\n",
            "Epoch: 31/50... Step: 340... Loss: 1.0343... Val Loss: 1.2779\n",
            "Epoch: 32/50... Step: 350... Loss: 1.0392... Val Loss: 1.2741\n",
            "Epoch: 33/50... Step: 360... Loss: 1.0382... Val Loss: 1.2741\n",
            "Epoch: 34/50... Step: 370... Loss: 1.0432... Val Loss: 1.2742\n",
            "Epoch: 35/50... Step: 380... Loss: 1.0378... Val Loss: 1.2735\n",
            "Epoch: 36/50... Step: 390... Loss: 1.0344... Val Loss: 1.2733\n",
            "Epoch: 37/50... Step: 400... Loss: 1.0284... Val Loss: 1.2756\n",
            "Epoch: 38/50... Step: 410... Loss: 1.0333... Val Loss: 1.2743\n",
            "Epoch: 39/50... Step: 420... Loss: 1.0332... Val Loss: 1.2765\n",
            "Epoch: 40/50... Step: 430... Loss: 1.0994... Val Loss: 1.2756\n",
            "Epoch: 40/50... Step: 440... Loss: 1.0841... Val Loss: 1.2775\n",
            "Epoch: 41/50... Step: 450... Loss: 1.0261... Val Loss: 1.2781\n",
            "Epoch: 42/50... Step: 460... Loss: 1.0311... Val Loss: 1.2765\n",
            "Epoch: 43/50... Step: 470... Loss: 1.0317... Val Loss: 1.2799\n",
            "Epoch: 44/50... Step: 480... Loss: 1.0358... Val Loss: 1.2791\n",
            "Epoch: 45/50... Step: 490... Loss: 1.0329... Val Loss: 1.2783\n",
            "Epoch: 46/50... Step: 500... Loss: 1.0246... Val Loss: 1.2780\n",
            "Epoch: 47/50... Step: 510... Loss: 1.0231... Val Loss: 1.2778\n",
            "Epoch: 48/50... Step: 520... Loss: 1.0271... Val Loss: 1.2788\n",
            "Epoch: 49/50... Step: 530... Loss: 1.0218... Val Loss: 1.2760\n",
            "Epoch: 50/50... Step: 540... Loss: 1.0880... Val Loss: 1.2787\n",
            "Epoch: 50/50... Step: 550... Loss: 1.0785... Val Loss: 1.2801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GbKf5sTw796F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prediction\n"
      ]
    },
    {
      "metadata": {
        "id": "5gla-nVi4fqz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def predict(net, char, h=None, top_k=None):\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = onehot(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        inputs = inputs.cuda()\n",
        "        h = tuple([each.data for each in h])\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        p = p.cpu()\n",
        "\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "            \n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "        return net.int2char[char], h\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W3C8YVNE8Abk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    net.cuda()\n",
        "    \n",
        "    net.eval()\n",
        "    \n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "461kYBSSGngO",
        "colab_type": "code",
        "outputId": "7a3f22ff-4897-49a9-b85b-d0664950379e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "print(sample(net, 1000, prime='He ', top_k=5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "He had spoken to her as he was already began again\n",
            "the tall there; and his heart should get a sense of things, she\n",
            "stroked her hand and told him, that he could not say what, he went up, that she\n",
            "writes to him the same society almost the complex on the same sour herself and the\n",
            "more and mere children of Vronsky, and he could not go on worn. The\n",
            "minute at the sight of her husband was a corrept to this subject. The\n",
            "sight of hind and walked out of his sense.\n",
            "\n",
            "\"You did not like some passion?\" said Anna, smiling, at her head was in\n",
            "silence.\n",
            "\n",
            "\"Ah! I don't want the sound of the whole person,\" said Levin with\n",
            "promising talk in her eyes, and as though he was ashamed of her\n",
            "soul, and went into a play taking her eyes off her, she was at from\n",
            "the same door. He was told him the child's expression which showed\n",
            "him immeniever this way one of the study had been surd the sound of the\n",
            "short presence of husband and tack, and an artesting for a man was the same\n",
            "significance to a conversation to his wife, and t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wq05eLTtOMJA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}